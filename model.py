# model.py
import streamlit as st
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Layer, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import numpy as np
import pandas as pd
import traceback
import os
import json
from sklearn.preprocessing import MinMaxScaler

from utils import display_log, calculate_metrics

try:
    import keras_tuner as kt
    KERAS_TUNER_AVAILABLE = True
except ImportError:
    KERAS_TUNER_AVAILABLE = False
    display_log("The 'keras_tuner' library is not installed. Hyperparameter tuning will be skipped.", "error")

class TransformerBlock(Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):
        super(TransformerBlock, self).__init__(**kwargs)
        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = Sequential([Dense(ff_dim, activation="relu"), Dense(embed_dim)])
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training=None):
        attn_output = self.att(inputs, inputs, training=training)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1, training=training)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

    def get_config(self):
        config = super(TransformerBlock, self).get_config()
        config.update({
            "embed_dim": self.embed_dim, "num_heads": self.num_heads,
            "ff_dim": self.ff_dim, "rate": self.rate,
        })
        return config

def build_lstm_model(input_shape: tuple, output_dim: int, hp=None, manual_params: dict = None, learning_rate: float = 0.001):
    display_log("üèóÔ∏è Building LSTM Model...", "info")
    model = Sequential()
    num_lstm_layers = (manual_params or {}).get('num_lstm_layers', 2)
    lstm_units_1 = (manual_params or {}).get('lstm_units_1', 100)
    dropout_1 = (manual_params or {}).get('dropout_1', 0.2)
    lstm_units_2 = (manual_params or {}).get('lstm_units_2', 50)
    dropout_2 = (manual_params or {}).get('dropout_2', 0.2)

    model.add(LSTM(lstm_units_1, return_sequences=num_lstm_layers > 1, input_shape=input_shape))
    model.add(Dropout(dropout_1))
    if num_lstm_layers > 1:
        model.add(LSTM(lstm_units_2, return_sequences=False))
        model.add(Dropout(dropout_2))
    
    # UPDATED: Output layer predicts the entire horizon
    model.add(Dense(output_dim))
    
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mean_squared_error')
    return model

def build_transformer_model(input_shape: tuple, output_dim: int, hp=None, manual_params: dict = None, learning_rate: float = 0.001):
    display_log("üèóÔ∏è Building Transformer Model...", "info")
    inputs = Input(shape=input_shape)
    x = inputs
    num_blocks = (manual_params or {}).get('num_transformer_blocks', 2)
    num_heads = (manual_params or {}).get('num_heads', 2)
    ff_dim = (manual_params or {}).get('ff_dim', 32)
    dropout_rate = (manual_params or {}).get('dropout_rate', 0.2)
    mlp_units_1 = (manual_params or {}).get('mlp_units_1', 64)
    
    embed_dim = input_shape[1]
    for _ in range(num_blocks):
        x = TransformerBlock(embed_dim, num_heads, ff_dim, rate=dropout_rate)(x)
    
    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    x = Dropout(dropout_rate)(x)
    x = Dense(mlp_units_1, activation="relu")(x)
    
    # UPDATED: Output layer predicts the entire horizon
    outputs = Dense(output_dim)(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mean_squared_error')
    return model

def build_hybrid_model(input_shape: tuple, output_dim: int, hp=None, manual_params: dict = None, learning_rate: float = 0.001):
    display_log("üèóÔ∏è Building Hybrid (LSTM + Transformer) Model...", "info")
    inputs = Input(shape=input_shape)
    lstm_units = (manual_params or {}).get('lstm_units', 64)
    lstm_dropout = (manual_params or {}).get('lstm_dropout', 0.2)
    num_blocks = (manual_params or {}).get('num_transformer_blocks', 1)
    num_heads = (manual_params or {}).get('num_heads', 2)
    ff_dim = (manual_params or {}).get('ff_dim', 32)
    
    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)
    lstm_out = Dropout(lstm_dropout)(lstm_out)
    
    x = lstm_out
    embed_dim_transformer = lstm_units
    for _ in range(num_blocks):
        x = TransformerBlock(embed_dim_transformer, num_heads, ff_dim)(x)
    
    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    
    # UPDATED: Output layer predicts the entire horizon
    outputs = Dense(output_dim)(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='mean_squared_error')
    return model

def train_model(model: tf.keras.Model, X_train: np.ndarray, y_train: np.ndarray,
                epochs: int, batch_size: int, learning_rate: float,
                X_val: np.ndarray = None, y_val: np.ndarray = None):
    display_log(f"üèãÔ∏è Starting model training for {epochs} epochs...", "info")
    model.optimizer.learning_rate.assign(learning_rate)
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=0)
    ]
    validation_data = (X_val, y_val) if X_val is not None and y_val is not None else None
    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0,
              callbacks=callbacks, validation_data=validation_data)
    display_log("‚úÖ Model training complete.", "info")

def predict_prices(model: tf.keras.Model, processed_data: pd.DataFrame, scaler: MinMaxScaler,
                   close_col_index: int, time_steps: int, prediction_horizon: int,
                   last_actual_values_before_diff: pd.Series = None,
                   was_differenced: bool = False):
    """
    Predicts a sequence of future stock prices using a direct multi-step forecasting model.
    """
    display_log(f"üîÆ Generating a direct {prediction_horizon}-day future forecast...", "info")
    try:
        last_sequence = processed_data.iloc[-time_steps:].values
        if last_sequence.shape[0] < time_steps:
            display_log("‚ùó Not enough data for future prediction.", "warning")
            return np.array([])

        input_reshaped = last_sequence.reshape(1, time_steps, last_sequence.shape[1])
        predicted_sequence_scaled = model.predict(input_reshaped, verbose=0)[0]

        dummy_array = np.zeros((prediction_horizon, scaler.n_features_in_))
        dummy_array[:, close_col_index] = predicted_sequence_scaled
        inverse_transformed_preds = scaler.inverse_transform(dummy_array)[:, close_col_index]

        if was_differenced:
            last_known_price = last_actual_values_before_diff.get('Close', 0)
            future_predictions = last_known_price + np.cumsum(inverse_transformed_preds)
        else:
            future_predictions = inverse_transformed_preds

        display_log("‚úÖ Direct future forecast generated.", "info")
        return future_predictions
    except Exception as e:
        display_log(f"‚ùå Error in direct future prediction: {e}", "error")
        return np.array([])

# Keras Tuner functions remain largely the same, as they call the updated model builders.
if KERAS_TUNER_AVAILABLE:
    def build_model_for_tuning(hp, model_type, input_shape, output_dim):
        if model_type == 'LSTM':
            return build_lstm_model(input_shape, output_dim, hp=hp)
        elif model_type == 'Transformer':
            return build_transformer_model(input_shape, output_dim, hp=hp)
        elif model_type == 'Hybrid (LSTM + Transformer)':
            return build_hybrid_model(input_shape, output_dim, hp=hp)
        else:
            raise ValueError(f"Unknown model type for tuning: {model_type}")

    def run_hyperparameter_tuning(model_type: str, input_shape: tuple, output_dim: int,
                                  X_train: np.ndarray, y_train: np.ndarray,
                                  num_trials: int, executions_per_trial: int,
                                  best_models_dir: str, force_retune: bool = False,
                                  epochs: int = 50):
        # This function's logic is okay, as it just orchestrates the tuning.
        pass # Placeholder for brevity, original logic is sound.

    def load_best_tuner_model(model_type: str, input_shape: tuple, output_dim: int, best_models_dir: str):
        # This function's logic is okay as well.
        pass # Placeholder for brevity, original logic is sound.
else:
    def run_hyperparameter_tuning(*args, **kwargs): return None, None, {}, {}
    def load_best_tuner_model(*args, **kwargs): return None, None, {}
